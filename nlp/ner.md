# NER

- https://towardsdatascience.com/pytorch-scene-text-detection-and-recognition-by-craft-and-a-four-stage-network-ec814d39db05
- https://github.com/courao/ocr.pytorch
- https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html
- https://nanonets.com/blog/attention-ocr-for-text-recogntion/
- https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr

# Annotator
https://bohemian.ai/blog/text-ant-annotation-in-nlp

- https://prodi.gy/
- https://www.tagtog.net/  
  - https://www.tagtog.net/#pdf-annotation
- https://inception-project.github.io/  
- https://github.com/doccano/doccano
- https://github.com/kamalkraj/BERT-NER
- https://brat.nlplab.org/
- https://www.lighttag.io/features/named-entity-recognition/ 
- https://github.com/tecoholic/ner-annotator
    - https://arunmozhi.in/2020/12/19/ner-annotator-ner-tagger-for-spacy/
    - https://github.com/amrrs/custom-ner-with-spacy
- https://github.com/Wadaboa/ner-annotator
- https://github.com/ManivannanMurugavel/spacy-ner-annotator
- https://www.npmjs.com/package/react-pdf-ner-annotator
- https://thinkinfi.com/prepare-training-data-for-custom-ner-using-webanno/


# Flask and Flaskrestplus
- https://towardsdatascience.com/working-with-apis-using-flask-flask-restplus-and-swagger-ui-7cf447deda7f



While dealing with #ML / #DL, we often come across Liner and Non-linear concept. 
Here is my summarization...

Linear Equation
- Forms a straight line on graph
- Maximum degree of a term is ONE
- Eg: y = mx + c
- Slope(m) is constant
- Has only one variable
- Output is proportional to input
- Eating chocolate and gaining weight is linear


Non-Linear Equation
- Forms a curve on graph
- Maximum degree of a term is > TWO
- Eg: ax^2 + by^2 = c
- Varying slope based on the location on the curve  
- Has only one variable
- Output and input are not directly related
- A cancer cell multiplication is not linear  

DL 
- Adding multiple LT layer/activation function leads to nothing but one added up weight and base matrix
- Adding multiple NLT layer/activation function helps in creating universal approximate functions(bag of non-linear functions) capturing the real world correlations 



# References
- https://stackoverflow.com/questions/9782071/why-must-a-nonlinear-activation-function-be-used-in-a-backpropagation-neural-net
- https://mathinsight.org/linear_transformation_definition_euclidean
  
Affine Transformation
- AT helps to modify the geometric structure of the image/matrix, preserving parallelism of lines but not the lengths and angles.
- Operations: Translate, Rotate, Scale, Shear  
- https://brilliant.org/wiki/affine-transformations/
- https://medium.com/mlait/affine-transformation-image-processing-in-tensorflow-part-1-df96256928a
